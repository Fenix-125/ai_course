{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.datasets import make_blobs, make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    # to simplify calculations we add ones column to data for multiplication with intercept\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    return np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "def vizualize_linear_regression(X, y, theta):\n",
    "    plt.scatter(X[:, 1], y, alpha=0.5)\n",
    "    x_axis = np.arange(np.min(X[:, 1]), np.max(X[:, 1]), 0.01).reshape(-1, 1)\n",
    "    y_axis = linear_predict(add_intercept(x_axis), theta)\n",
    "    plt.plot(x_axis, y_axis, lw=2, c=\"r\")\n",
    "    plt.show()\n",
    "\n",
    "def vizualize_logistic_regression(X, y, theta, treshold=0.5):\n",
    "    plt.scatter(X[:, 1], X[:, 2], c=y, alpha=0.5)\n",
    "    eps = 0.00000001\n",
    "    x_axis = np.arange(np.min(X[:, 1]), np.max(X[:, 1]), 0.01).reshape(-1, 1)\n",
    "    y_axis =  - (np.log((1-treshold)/treshold) + add_intercept(x_axis).dot(theta[:2])) / (theta[2] + eps)\n",
    "    plt.plot(x_axis, y_axis, lw=2, c=\"r\")\n",
    "    plt.show()\n",
    "    \n",
    "def data_generator_linear_regression(n=100, intercept=0, coef=np.array([1]), var=0.1):\n",
    "    X = np.random.rand(n, coef.shape[0]) \n",
    "    theta = np.insert(coef, 0, intercept)\n",
    "    y = add_intercept(X).dot(theta) + np.random.normal(0, var, (n))\n",
    "    return X, y\n",
    "\n",
    "def data_generator_logistic_regression(n=100, data=\"blobs\"):\n",
    "    if data == \"blobs\":\n",
    "        X, y = make_blobs(n, centers=2)\n",
    "    elif data == \"moons\":\n",
    "        X, y = make_moons(n)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2 points) Implement predictor for linear regression:\n",
    "$$ h_\\theta (X) = \\theta^TX $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_predict(X, theta):\n",
    "    return theta.transpose().dot(X.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3 points) Implement linear cost function:\n",
    "$$ J(\\theta) = \\dfrac{1}{2n}\\sum_{i=1}^{n}(h_\\theta (X^{(i)})-y^{(i)})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_cost(X, y, theta):\n",
    "    return 1 / (2 * len(X)) * np.sum(np.power(linear_predict(X, theta) - y, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3 points) Implement gradient function for linear regression:\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\dfrac{1}{n}\\sum_{i=1}^{n}\\left( h_\\theta(x^{(i)}) - y^{(i)} \\right) x^{(i)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(X, y, theta):\n",
    "    res = 0\n",
    "    for index in range(len(X)):\n",
    "        res += (linear_predict(X[index], theta) - y[index]) * X[index]\n",
    "    return 1 / len(X) * res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2 points) Implement weights initialization and weights update:\n",
    "$$ \\theta = \\theta - \\alpha \\nabla_\\theta J(\\theta) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_linear(X, y, lr=2, max_iter=10, epsilon=0.01, visualize=False):\n",
    "    X = add_intercept(X)\n",
    "    # randomly initialize weights vector with ones coresponding to X shape\n",
    "    theta = np.random.binomial(len(X[0]), .5, (len(X[0])))# Your code hear\n",
    "    print(theta)\n",
    "    cost = linear_cost(X, y, theta) #################################\n",
    "    cost_list = [cost]\n",
    "    for i in range(max_iter):\n",
    "        # update values of weights based on gradient\n",
    "        theta -= lr * linear_gradient(X, y, theta) * cost# Your code hear\n",
    "        cost = linear_cost(X, y, theta)\n",
    "        cost_list.append(cost)\n",
    "        print(cost)\n",
    "        if visualize == True:\n",
    "            time.sleep(0.1)\n",
    "            clear_output(wait=True)\n",
    "            vizualize_linear_regression(X, y, theta)\n",
    "        \n",
    "        if np.abs(cost_list[-1] - cost_list[-2]) < epsilon:\n",
    "            break \n",
    "    print(\"theta\", theta)\n",
    "    print(\"cost\", cost)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2 points) Try different parameters of lr and max_iter, what is optimal value? \n",
    "Experiment with different data generators.  \n",
    "Write short summary on experiments.  \n",
    "(In case of single dimensional data you can use vizualization.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n",
      "[1 1]\n",
      "[1.         0.84523875]\n",
      "[1 1]\n",
      "[1.         0.12063291]\n",
      "[1 1]\n",
      "[1.         0.73263223]\n",
      "[1 1]\n",
      "[1.         0.97239469]\n",
      "[1 1]\n",
      "[1.         0.92657961]\n",
      "[1 1]\n",
      "[1.         0.00878429]\n",
      "[1 1]\n",
      "[1.         0.45885877]\n",
      "[1 1]\n",
      "[1.         0.94131367]\n",
      "[1 1]\n",
      "[1.         0.57301817]\n",
      "[1 1]\n",
      "[1.         0.96770059]\n",
      "[1 1]\n",
      "[1.         0.02393053]\n",
      "[1 1]\n",
      "[1.         0.64804311]\n",
      "[1 1]\n",
      "[1.         0.22383842]\n",
      "[1 1]\n",
      "[1.        0.3354509]\n",
      "[1 1]\n",
      "[1.         0.99064424]\n",
      "[1 1]\n",
      "[1.         0.70071614]\n",
      "[1 1]\n",
      "[1.         0.97534152]\n",
      "[1 1]\n",
      "[1.         0.81358183]\n",
      "[1 1]\n",
      "[1.        0.0247395]\n",
      "[1 1]\n",
      "[1.         0.66719776]\n",
      "[1 1]\n",
      "[1.         0.47767535]\n",
      "[1 1]\n",
      "[1.         0.67326811]\n",
      "[1 1]\n",
      "[1.         0.04490161]\n",
      "[1 1]\n",
      "[1.         0.22143023]\n",
      "[1 1]\n",
      "[1.         0.48356311]\n",
      "[1 1]\n",
      "[1.         0.53528319]\n",
      "[1 1]\n",
      "[1.         0.84571282]\n",
      "[1 1]\n",
      "[1.         0.39630357]\n",
      "[1 1]\n",
      "[1.         0.47089577]\n",
      "[1 1]\n",
      "[1.         0.63214359]\n",
      "[1 1]\n",
      "[1.         0.15043492]\n",
      "[1 1]\n",
      "[1.        0.5737978]\n",
      "[1 1]\n",
      "[1.         0.87627883]\n",
      "[1 1]\n",
      "[1.         0.44592231]\n",
      "[1 1]\n",
      "[1.         0.93490855]\n",
      "[1 1]\n",
      "[1.         0.89567492]\n",
      "[1 1]\n",
      "[1.         0.91353683]\n",
      "[1 1]\n",
      "[1.        0.1329068]\n",
      "[1 1]\n",
      "[1.         0.07639564]\n",
      "[1 1]\n",
      "[1.         0.77162394]\n",
      "[1 1]\n",
      "[1.         0.45264478]\n",
      "[1 1]\n",
      "[1.        0.8299556]\n",
      "[1 1]\n",
      "[1.         0.11006976]\n",
      "[1 1]\n",
      "[1.         0.25038891]\n",
      "[1 1]\n",
      "[1.         0.94316213]\n",
      "[1 1]\n",
      "[1.         0.19326506]\n",
      "[1 1]\n",
      "[1.         0.42440146]\n",
      "[1 1]\n",
      "[1.         0.92316391]\n",
      "[1 1]\n",
      "[1.         0.09109452]\n",
      "[1 1]\n",
      "[1.         0.11459834]\n",
      "[1 1]\n",
      "[1.         0.79624589]\n",
      "[1 1]\n",
      "[1.         0.34359722]\n",
      "[1 1]\n",
      "[1.         0.24886472]\n",
      "[1 1]\n",
      "[1.         0.70182606]\n",
      "[1 1]\n",
      "[1.         0.29082417]\n",
      "[1 1]\n",
      "[1.         0.95297018]\n",
      "[1 1]\n",
      "[1.         0.12487283]\n",
      "[1 1]\n",
      "[1.         0.14817644]\n",
      "[1 1]\n",
      "[1.         0.59192723]\n",
      "[1 1]\n",
      "[1.        0.2862365]\n",
      "[1 1]\n",
      "[1.         0.75979885]\n",
      "[1 1]\n",
      "[1.         0.43501811]\n",
      "[1 1]\n",
      "[1.         0.86748393]\n",
      "[1 1]\n",
      "[1.         0.33252952]\n",
      "[1 1]\n",
      "[1.         0.02310634]\n",
      "[1 1]\n",
      "[1.         0.11285971]\n",
      "[1 1]\n",
      "[1.         0.52714725]\n",
      "[1 1]\n",
      "[1.         0.11989967]\n",
      "[1 1]\n",
      "[1.         0.36811426]\n",
      "[1 1]\n",
      "[1.         0.53619464]\n",
      "[1 1]\n",
      "[1.         0.30912559]\n",
      "[1 1]\n",
      "[1.         0.66483234]\n",
      "[1 1]\n",
      "[1.         0.17537789]\n",
      "[1 1]\n",
      "[1.         0.80139643]\n",
      "[1 1]\n",
      "[1.        0.2552939]\n",
      "[1 1]\n",
      "[1.         0.41833005]\n",
      "[1 1]\n",
      "[1.         0.42509718]\n",
      "[1 1]\n",
      "[1.         0.95962311]\n",
      "[1 1]\n",
      "[1.         0.45209894]\n",
      "[1 1]\n",
      "[1.         0.43864653]\n",
      "[1 1]\n",
      "[1.         0.63390334]\n",
      "[1 1]\n",
      "[1.         0.07009116]\n",
      "[1 1]\n",
      "[1.      0.09206]\n",
      "[1 1]\n",
      "[1.         0.12808243]\n",
      "[1 1]\n",
      "[1.         0.50428836]\n",
      "[1 1]\n",
      "[1.         0.48089899]\n",
      "[1 1]\n",
      "[1.         0.05075586]\n",
      "[1 1]\n",
      "[1.         0.18367997]\n",
      "[1 1]\n",
      "[1.         0.72860219]\n",
      "[1 1]\n",
      "[1.         0.97676782]\n",
      "[1 1]\n",
      "[1.         0.59078763]\n",
      "[1 1]\n",
      "[1.         0.96875857]\n",
      "[1 1]\n",
      "[1.         0.39990896]\n",
      "[1 1]\n",
      "[1.         0.11660412]\n",
      "[1 1]\n",
      "[1.        0.6233613]\n",
      "[1 1]\n",
      "[1.         0.59297662]\n",
      "[1 1]\n",
      "[1.         0.30410164]\n",
      "[1 1]\n",
      "[1.         0.84444127]\n",
      "[1 1]\n",
      "[1.         0.07227691]\n",
      "[1 1]\n",
      "[1.         0.29645429]\n",
      "[1 1]\n",
      "[1.         0.84523875]\n",
      "[1 1]\n",
      "[1.         0.12063291]\n",
      "[1 1]\n",
      "[1.         0.73263223]\n",
      "[1 1]\n",
      "[1.         0.97239469]\n",
      "[1 1]\n",
      "[1.         0.92657961]\n",
      "[1 1]\n",
      "[1.         0.00878429]\n",
      "[1 1]\n",
      "[1.         0.45885877]\n",
      "[1 1]\n",
      "[1.         0.94131367]\n",
      "[1 1]\n",
      "[1.         0.57301817]\n",
      "[1 1]\n",
      "[1.         0.96770059]\n",
      "[1 1]\n",
      "[1.         0.02393053]\n",
      "[1 1]\n",
      "[1.         0.64804311]\n",
      "[1 1]\n",
      "[1.         0.22383842]\n",
      "[1 1]\n",
      "[1.        0.3354509]\n",
      "[1 1]\n",
      "[1.         0.99064424]\n",
      "[1 1]\n",
      "[1.         0.70071614]\n",
      "[1 1]\n",
      "[1.         0.97534152]\n",
      "[1 1]\n",
      "[1.         0.81358183]\n",
      "[1 1]\n",
      "[1.        0.0247395]\n",
      "[1 1]\n",
      "[1.         0.66719776]\n",
      "[1 1]\n",
      "[1.         0.47767535]\n",
      "[1 1]\n",
      "[1.         0.67326811]\n",
      "[1 1]\n",
      "[1.         0.04490161]\n",
      "[1 1]\n",
      "[1.         0.22143023]\n",
      "[1 1]\n",
      "[1.         0.48356311]\n",
      "[1 1]\n",
      "[1.         0.53528319]\n",
      "[1 1]\n",
      "[1.         0.84571282]\n",
      "[1 1]\n",
      "[1.         0.39630357]\n",
      "[1 1]\n",
      "[1.         0.47089577]\n",
      "[1 1]\n",
      "[1.         0.63214359]\n",
      "[1 1]\n",
      "[1.         0.15043492]\n",
      "[1 1]\n",
      "[1.        0.5737978]\n",
      "[1 1]\n",
      "[1.         0.87627883]\n",
      "[1 1]\n",
      "[1.         0.44592231]\n",
      "[1 1]\n",
      "[1.         0.93490855]\n",
      "[1 1]\n",
      "[1.         0.89567492]\n",
      "[1 1]\n",
      "[1.         0.91353683]\n",
      "[1 1]\n",
      "[1.        0.1329068]\n",
      "[1 1]\n",
      "[1.         0.07639564]\n",
      "[1 1]\n",
      "[1.         0.77162394]\n",
      "[1 1]\n",
      "[1.         0.45264478]\n",
      "[1 1]\n",
      "[1.        0.8299556]\n",
      "[1 1]\n",
      "[1.         0.11006976]\n",
      "[1 1]\n",
      "[1.         0.25038891]\n",
      "[1 1]\n",
      "[1.         0.94316213]\n",
      "[1 1]\n",
      "[1.         0.19326506]\n",
      "[1 1]\n",
      "[1.         0.42440146]\n",
      "[1 1]\n",
      "[1.         0.92316391]\n",
      "[1 1]\n",
      "[1.         0.09109452]\n",
      "[1 1]\n",
      "[1.         0.11459834]\n",
      "[1 1]\n",
      "[1.         0.79624589]\n",
      "[1 1]\n",
      "[1.         0.34359722]\n",
      "[1 1]\n",
      "[1.         0.24886472]\n",
      "[1 1]\n",
      "[1.         0.70182606]\n",
      "[1 1]\n",
      "[1.         0.29082417]\n",
      "[1 1]\n",
      "[1.         0.95297018]\n",
      "[1 1]\n",
      "[1.         0.12487283]\n",
      "[1 1]\n",
      "[1.         0.14817644]\n",
      "[1 1]\n",
      "[1.         0.59192723]\n",
      "[1 1]\n",
      "[1.        0.2862365]\n",
      "[1 1]\n",
      "[1.         0.75979885]\n",
      "[1 1]\n",
      "[1.         0.43501811]\n",
      "[1 1]\n",
      "[1.         0.86748393]\n",
      "[1 1]\n",
      "[1.         0.33252952]\n",
      "[1 1]\n",
      "[1.         0.02310634]\n",
      "[1 1]\n",
      "[1.         0.11285971]\n",
      "[1 1]\n",
      "[1.         0.52714725]\n",
      "[1 1]\n",
      "[1.         0.11989967]\n",
      "[1 1]\n",
      "[1.         0.36811426]\n",
      "[1 1]\n",
      "[1.         0.53619464]\n",
      "[1 1]\n",
      "[1.         0.30912559]\n",
      "[1 1]\n",
      "[1.         0.66483234]\n",
      "[1 1]\n",
      "[1.         0.17537789]\n",
      "[1 1]\n",
      "[1.         0.80139643]\n",
      "[1 1]\n",
      "[1.        0.2552939]\n",
      "[1 1]\n",
      "[1.         0.41833005]\n",
      "[1 1]\n",
      "[1.         0.42509718]\n",
      "[1 1]\n",
      "[1.         0.95962311]\n",
      "[1 1]\n",
      "[1.         0.45209894]\n",
      "[1 1]\n",
      "[1.         0.43864653]\n",
      "[1 1]\n",
      "[1.         0.63390334]\n",
      "[1 1]\n",
      "[1.         0.07009116]\n",
      "[1 1]\n",
      "[1.      0.09206]\n",
      "[1 1]\n",
      "[1.         0.12808243]\n",
      "[1 1]\n",
      "[1.         0.50428836]\n",
      "[1 1]\n",
      "[1.         0.48089899]\n",
      "[1 1]\n",
      "[1.         0.05075586]\n",
      "[1 1]\n",
      "[1.         0.18367997]\n",
      "[1 1]\n",
      "[1.         0.72860219]\n",
      "[1 1]\n",
      "[1.         0.97676782]\n",
      "[1 1]\n",
      "[1.         0.59078763]\n",
      "[1 1]\n",
      "[1.         0.96875857]\n",
      "[1 1]\n",
      "[1.         0.39990896]\n",
      "[1 1]\n",
      "[1.         0.11660412]\n",
      "[1 1]\n",
      "[1.        0.6233613]\n",
      "[1 1]\n",
      "[1.         0.59297662]\n",
      "[1 1]\n",
      "[1.         0.30410164]\n",
      "[1 1]\n",
      "[1.         0.84444127]\n",
      "[1 1]\n",
      "[1.         0.07227691]\n",
      "[1 1]\n",
      "[1.         0.29645429]\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "Cannot cast ufunc 'subtract' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUFuncTypeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-53-aa113f4ea6a8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata_generator_linear_regression\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mfit_linear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvisualize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-52-b9658baf1a2d>\u001B[0m in \u001B[0;36mfit_linear\u001B[0;34m(X, y, lr, max_iter, epsilon, visualize)\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_iter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0;31m# update values of weights based on gradient\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m         \u001B[0mtheta\u001B[0m \u001B[0;34m-=\u001B[0m \u001B[0mlr\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mlinear_gradient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtheta\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mcost\u001B[0m\u001B[0;31m# Your code hear\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m         \u001B[0mcost\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlinear_cost\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtheta\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m         \u001B[0mcost_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcost\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mUFuncTypeError\u001B[0m: Cannot cast ufunc 'subtract' output from dtype('float64') to dtype('int64') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "X, y = data_generator_linear_regression()\n",
    "fit_linear(X, y, lr=1, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4 points) Try to find coeficients just with linear algebra toolbox instead of optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squere(X, y):\n",
    "    return np.linalg.lstsq(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2 points) Compare precision of results and time of execution of least squered and optimization solutions  \n",
    "Tip: Try to use *%timeit* from ipython magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-6b6264c0d8c8>:2: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  return np.linalg.lstsq(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.6 ms ± 1.55 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "25.5 ms ± 588 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "        [1, -0.5, 3, 1],\n",
    "        [1, 8, -0.33, 5],\n",
    "        [1, 0, 0, 0]\n",
    "    ])\n",
    "y = np.array([40, 100, 12])\n",
    "\n",
    "%timeit for _ in range(1000): least_squere(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1 point) Implement sigmoid function:\n",
    "$$ \\sigma (z) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-z}}  $$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.        , 1.        , 0.99999386])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vizualize sigmoid to check your code:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def vizualize_sigmoid(range_min=-10, range_max=10):\n",
    "    x_axis = np.linspace(range_min,range_max,100)   \n",
    "    y_axis = sigmoid(x_axis)\n",
    "    plt.plot(x_axis,y_axis)\n",
    "    plt.show()\n",
    "\n",
    "vizualize_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1 point) Implement predictor for logistic regression:\n",
    "$$ h_\\theta (X) = \\sigma (\\theta^TX) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_predict(X, theta):\n",
    "    return sigmoid(linear_predict(X, theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2 points) Implement cross entropy cost function:\n",
    "$$ J(\\theta) = -\\dfrac{1}{n}\\sum_{i=1}^{n}(y^{(i)}*log(h_\\theta (X^{(i)}))+(1−y^{(i)})*log(1−h_\\theta (X^{(i)}))) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cost(X, y, theta):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4 points) Calculate derivative from cross entropy and implement gradient step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_gradient(X, y, theta):\n",
    "    # your code hear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2 points) Similarly to previous example implement training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic(X, y, lr=2, max_iter=10, epsilon=0.0001, visualize=False):\n",
    "    # your code hear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2 points) Play with different parameters of lr and max_iter, try different datasets.  \n",
    "Does algorithm always find optimal line for separation?   \n",
    "What is a problem? How should optimal line look like to your mind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = data_generator_logistic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_logistic(X,y, lr=1, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class LinearRegresionTests(unittest.TestCase):\n",
    "\n",
    "    X = np.array([\n",
    "        [1, -0.5, 3, 1],\n",
    "        [1, 8, -0.33, 5],\n",
    "        [1, 0, 0, 0]\n",
    "    ])\n",
    "    y = np.array([40, 100, 12])\n",
    "    y_bin = np.array([0, 1, 0])\n",
    "    theta = np.array([2, 5, 7, 9])\n",
    "    eps = 0.001\n",
    "\n",
    "    def assertFloatEquals(self, a, b):\n",
    "        self.assertTrue(np.abs(a - b) < self.eps)\n",
    "    \n",
    "    def assertArrayEquals(self, a, b):\n",
    "        a = np.array(a)\n",
    "        b = np.array(b)\n",
    "        self.assertEqual(a.shape, b.shape)\n",
    "        self.assertTrue(np.all(np.abs(a - b) < self.eps))\n",
    "    \n",
    "    def test_linear_predict_for_single_example(self):\n",
    "        expected = 29.5\n",
    "        actual = linear_predict(self.X[0], self.theta)\n",
    "        self.assertFloatEquals(actual, expected)\n",
    "    \n",
    "    def test_linear_predict_for_multiple_examples(self):\n",
    "        expected = [29.5, 84.69, 2.]\n",
    "        actual = linear_predict(self.X, self.theta)\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "        \n",
    "    def test_linear_cost(self):\n",
    "        expected = 74.107\n",
    "        actual = linear_cost(self.X, self.y, self.theta)\n",
    "        self.assertFloatEquals(actual, expected)\n",
    "        \n",
    "    def test_linear_gradient(self):\n",
    "        expected = [-11.936, -39.076, -8.815, -29.016]\n",
    "        actual = linear_gradient(self.X, self.y, self.theta)\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "    \n",
    "    def test_sigmoid(self):\n",
    "        expected = [1., 1., 0.999]\n",
    "        actual = sigmoid(self.y)\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "    \n",
    "    def test_logistic_predict_for_single_example(self):\n",
    "        expected = 0.88\n",
    "        actual = logistic_predict(self.X[2], self.theta)\n",
    "        self.assertFloatEquals(actual, expected)\n",
    "    \n",
    "    def test_logistic_predict_for_multiple_examples(self):\n",
    "        expected = [1, 1, 0.88]\n",
    "        actual = logistic_predict(self.X, self.theta)\n",
    "        self.assertArrayEquals(actual, expected)\n",
    "        \n",
    "    def test_logistic_cost(self):\n",
    "        expected = 4.546\n",
    "        actual = logistic_cost(self.X, self.y_bin, self.theta)\n",
    "        self.assertFloatEquals(actual, expected)\n",
    "        \n",
    "    def test_logistic_gradient(self):\n",
    "        expected = [0.626, -0.166,  1, 0.333]\n",
    "        actual = logistic_gradient(self.X, self.y_bin, self.theta)\n",
    "        self.assertArrayEquals(actual, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_linear_cost (__main__.LinearRegresionTests) ... ok\n",
      "test_linear_gradient (__main__.LinearRegresionTests) ... ok\n",
      "test_linear_predict_for_multiple_examples (__main__.LinearRegresionTests) ... ok\n",
      "test_linear_predict_for_single_example (__main__.LinearRegresionTests) ... ok\n",
      "test_logistic_cost (__main__.LinearRegresionTests) ... ERROR\n",
      "test_logistic_gradient (__main__.LinearRegresionTests) ... ERROR\n",
      "test_logistic_predict_for_multiple_examples (__main__.LinearRegresionTests) ... ok\n",
      "test_logistic_predict_for_single_example (__main__.LinearRegresionTests) ... ok\n",
      "test_sigmoid (__main__.LinearRegresionTests) ... ok\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_logistic_cost (__main__.LinearRegresionTests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-71-1edfbe6eefa8>\", line 61, in test_logistic_cost\n",
      "    actual = logistic_cost(self.X, self.y_bin, self.theta)\n",
      "NameError: name 'logistic_cost' is not defined\n",
      "\n",
      "======================================================================\n",
      "ERROR: test_logistic_gradient (__main__.LinearRegresionTests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-71-1edfbe6eefa8>\", line 66, in test_logistic_gradient\n",
      "    actual = logistic_gradient(self.X, self.y_bin, self.theta)\n",
      "NameError: name 'logistic_gradient' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 0.015s\n",
      "\n",
      "FAILED (errors=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5 7 9]\n",
      "[[ 1.   -0.5   3.    1.  ]\n",
      " [ 1.    8.   -0.33  5.  ]\n",
      " [ 1.    0.    0.    0.  ]]\n",
      "[2 5 7 9]\n",
      "[ 1.  -0.5  3.   1. ]\n",
      "[2 5 7 9]\n",
      "[ 1.    8.   -0.33  5.  ]\n",
      "[2 5 7 9]\n",
      "[1. 0. 0. 0.]\n",
      "[2 5 7 9]\n",
      "[[ 1.   -0.5   3.    1.  ]\n",
      " [ 1.    8.   -0.33  5.  ]\n",
      " [ 1.    0.    0.    0.  ]]\n",
      "[2 5 7 9]\n",
      "[ 1.  -0.5  3.   1. ]\n",
      "[2 5 7 9]\n",
      "[[ 1.   -0.5   3.    1.  ]\n",
      " [ 1.    8.   -0.33  5.  ]\n",
      " [ 1.    0.    0.    0.  ]]\n",
      "[2 5 7 9]\n",
      "[1. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x7fd7e2c95910>"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9bca0d8",
   "language": "python",
   "display_name": "PyCharm (lab_1)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}