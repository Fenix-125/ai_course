{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight network architecture for the Mnist dataset (digit) classification\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MnistNet, self).__init__()\n",
    "        self.num_classes = 10\n",
    "        \n",
    "        # fully convolutional part\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=5),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(4, 4, kernel_size=5),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ReLU(inplace=True)            \n",
    "        )\n",
    "        \n",
    "        # classifier, FC layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(16*4,16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16,self.num_classes),\n",
    "            nn.BatchNorm1d(self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x.view(-1,x.size(-3)*x.size(-2)*x.size(-1)))\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer):\n",
    "    \"\"\"\n",
    "    Training of an epoch\n",
    "    model: network\n",
    "    train_loader: train_loader loading images and labels in batches\n",
    "    optimizer: optimizer to use in the training\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_total = 0\n",
    "    train_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad() # zero the accumulated gradients\n",
    "        output = model(data) # computer network's output\n",
    "        loss = F.cross_entropy(output, target) # computer the loss\n",
    "        loss.backward() # backward pass\n",
    "        optimizer.step() # update weights\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        scores, predictions = torch.max(output.data, 1)\n",
    "        train_total += target.size(0)\n",
    "        train_correct += int(sum((1 for pr, val in zip(predictions, target) if pr == val)))\n",
    "        acc = (train_correct / train_total) * 100\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('[{}/{} ({:.0f}%)] Training\\tBatch loss: {:.6f}\\tAccuracy: {:.6f}%'.format(\n",
    "                 batch_idx * len(data), len(train_loader.dataset),\n",
    "                 100. * batch_idx / len(train_loader), loss.item()/len(data),\n",
    "                 acc))\n",
    "    \n",
    "    print('Training: Epoch average loss {:.6f}'.format(total_loss/len(train_loader.dataset)),\n",
    "          'Epoch accuracy {:.6f}%'.format((train_correct / train_total) * 100))\n",
    "         \n",
    "        \n",
    "def test(model, val_loader):\n",
    "    \"\"\"\n",
    "    Compute accuracy on the validation set\n",
    "    model: network\n",
    "    val_loader: test_loader loading images and labels in batches\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # implement validation procedure, report accuracy on the validation set\n",
    "\n",
    "    total_loss = 0\n",
    "    val_total = 0\n",
    "    val_correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        total_loss += loss.item()*data.size(0)\n",
    "        scores, predictions = torch.max(output.data, 1)\n",
    "        val_total += target.size(0)\n",
    "        val_correct += int(sum((1 for pr, val in zip(predictions, target) if pr == val)))\n",
    "        val_acc = (val_correct / val_total) * 100\n",
    "\n",
    "        print('[{}/{} ({:.0f}%)] Validation\\tBatch loss: {:.6f}\\tAccuracy: {:.6f}%'.format(\n",
    "             batch_idx * len(data), len(val_loader.dataset),\n",
    "             100. * batch_idx / len(val_loader), loss.item()/len(data),\n",
    "             val_acc))\n",
    "\n",
    "    print('Validation: Epoch average loss {:.6f}\\tAccuracy: {:.6f}%'\n",
    "          .format(total_loss/len(val_loader.dataset), (val_correct / val_total) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample images\n"
     ]
    }
   ],
   "source": [
    "# mnist dataset structure - train part\n",
    "mnist_dataset_train = datasets.MNIST('vs3ex1data/mnist_data', train=True, transform=transforms.Compose([\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "               ]), download=True)\n",
    "# mnist dataset structure - test part\n",
    "mnist_dataset_val = datasets.MNIST('vs3ex1data/mnist_data', train=False, transform=transforms.Compose([\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "               ]), download=True)\n",
    "\n",
    "# show sample images\n",
    "print('Sample images')\n",
    "for i in range(0,100,10):\n",
    "    # plt.imshow(Image.fromarray(mnist_dataset_train.train_data[i].numpy(), mode='L'))\n",
    "    # plt.show()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "[0/60000 (0%)] Training\tBatch loss: 0.168874\tAccuracy: 6.250000%\n",
      "[1600/60000 (3%)] Training\tBatch loss: 0.093244\tAccuracy: 41.460396%\n",
      "[3200/60000 (5%)] Training\tBatch loss: 0.081196\tAccuracy: 47.388060%\n",
      "[4800/60000 (8%)] Training\tBatch loss: 0.040712\tAccuracy: 52.076412%\n",
      "[6400/60000 (11%)] Training\tBatch loss: 0.066294\tAccuracy: 55.174564%\n",
      "[8000/60000 (13%)] Training\tBatch loss: 0.052844\tAccuracy: 57.834331%\n",
      "[9600/60000 (16%)] Training\tBatch loss: 0.067780\tAccuracy: 59.608985%\n",
      "[11200/60000 (19%)] Training\tBatch loss: 0.038014\tAccuracy: 60.975392%\n",
      "[12800/60000 (21%)] Training\tBatch loss: 0.057234\tAccuracy: 62.406367%\n",
      "[14400/60000 (24%)] Training\tBatch loss: 0.062469\tAccuracy: 63.804107%\n",
      "[16000/60000 (27%)] Training\tBatch loss: 0.037988\tAccuracy: 65.047453%\n",
      "[17600/60000 (29%)] Training\tBatch loss: 0.049807\tAccuracy: 66.252271%\n",
      "[19200/60000 (32%)] Training\tBatch loss: 0.054822\tAccuracy: 67.162781%\n",
      "[20800/60000 (35%)] Training\tBatch loss: 0.056995\tAccuracy: 67.986164%\n",
      "[22400/60000 (37%)] Training\tBatch loss: 0.048312\tAccuracy: 68.830300%\n",
      "[24000/60000 (40%)] Training\tBatch loss: 0.061618\tAccuracy: 69.487009%\n",
      "[25600/60000 (43%)] Training\tBatch loss: 0.029539\tAccuracy: 70.085103%\n",
      "[27200/60000 (45%)] Training\tBatch loss: 0.043939\tAccuracy: 70.627572%\n",
      "[28800/60000 (48%)] Training\tBatch loss: 0.048629\tAccuracy: 71.206968%\n",
      "[30400/60000 (51%)] Training\tBatch loss: 0.044917\tAccuracy: 71.699106%\n",
      "[32000/60000 (53%)] Training\tBatch loss: 0.028942\tAccuracy: 72.182659%\n",
      "[33600/60000 (56%)] Training\tBatch loss: 0.042336\tAccuracy: 72.643979%\n",
      "[35200/60000 (59%)] Training\tBatch loss: 0.069881\tAccuracy: 72.998069%\n",
      "[36800/60000 (61%)] Training\tBatch loss: 0.026300\tAccuracy: 73.394720%\n",
      "[38400/60000 (64%)] Training\tBatch loss: 0.039452\tAccuracy: 73.802582%\n",
      "[40000/60000 (67%)] Training\tBatch loss: 0.064752\tAccuracy: 74.070372%\n",
      "[41600/60000 (69%)] Training\tBatch loss: 0.057794\tAccuracy: 74.392061%\n",
      "[43200/60000 (72%)] Training\tBatch loss: 0.017806\tAccuracy: 74.703813%\n",
      "[44800/60000 (75%)] Training\tBatch loss: 0.039607\tAccuracy: 74.986612%\n",
      "[46400/60000 (77%)] Training\tBatch loss: 0.020278\tAccuracy: 75.305929%\n",
      "[48000/60000 (80%)] Training\tBatch loss: 0.029714\tAccuracy: 75.620626%\n",
      "[49600/60000 (83%)] Training\tBatch loss: 0.030484\tAccuracy: 75.904950%\n",
      "[51200/60000 (85%)] Training\tBatch loss: 0.039355\tAccuracy: 76.120744%\n",
      "[52800/60000 (88%)] Training\tBatch loss: 0.045952\tAccuracy: 76.351863%\n",
      "[54400/60000 (91%)] Training\tBatch loss: 0.031239\tAccuracy: 76.637386%\n",
      "[56000/60000 (93%)] Training\tBatch loss: 0.030820\tAccuracy: 76.847686%\n",
      "[57600/60000 (96%)] Training\tBatch loss: 0.018534\tAccuracy: 77.060192%\n",
      "[59200/60000 (99%)] Training\tBatch loss: 0.027516\tAccuracy: 77.203796%\n",
      "Training: Epoch average loss 0.045525 Epoch accuracy 77.326667%\n",
      "[0/10000 (0%)] Validation\tBatch loss: 0.000354\tAccuracy: 95.117188%\n",
      "[512/10000 (5%)] Validation\tBatch loss: 0.000457\tAccuracy: 94.335938%\n",
      "[1024/10000 (10%)] Validation\tBatch loss: 0.000522\tAccuracy: 93.554688%\n",
      "[1536/10000 (15%)] Validation\tBatch loss: 0.000482\tAccuracy: 93.505859%\n",
      "[2048/10000 (20%)] Validation\tBatch loss: 0.000463\tAccuracy: 93.359375%\n",
      "[2560/10000 (25%)] Validation\tBatch loss: 0.000442\tAccuracy: 93.326823%\n",
      "[3072/10000 (30%)] Validation\tBatch loss: 0.000427\tAccuracy: 93.443080%\n",
      "[3584/10000 (35%)] Validation\tBatch loss: 0.000517\tAccuracy: 93.408203%\n",
      "[4096/10000 (40%)] Validation\tBatch loss: 0.000434\tAccuracy: 93.381076%\n",
      "[4608/10000 (45%)] Validation\tBatch loss: 0.000371\tAccuracy: 93.476562%\n",
      "[5120/10000 (50%)] Validation\tBatch loss: 0.000187\tAccuracy: 93.767756%\n",
      "[5632/10000 (55%)] Validation\tBatch loss: 0.000411\tAccuracy: 93.750000%\n",
      "[6144/10000 (60%)] Validation\tBatch loss: 0.000334\tAccuracy: 93.870192%\n",
      "[6656/10000 (65%)] Validation\tBatch loss: 0.000173\tAccuracy: 94.196429%\n",
      "[7168/10000 (70%)] Validation\tBatch loss: 0.000197\tAccuracy: 94.453125%\n",
      "[7680/10000 (75%)] Validation\tBatch loss: 0.000221\tAccuracy: 94.628906%\n",
      "[8192/10000 (80%)] Validation\tBatch loss: 0.000221\tAccuracy: 94.749540%\n",
      "[8704/10000 (85%)] Validation\tBatch loss: 0.000235\tAccuracy: 94.889323%\n",
      "[9216/10000 (90%)] Validation\tBatch loss: 0.000205\tAccuracy: 95.045230%\n",
      "[5168/10000 (95%)] Validation\tBatch loss: 0.001052\tAccuracy: 94.940000%\n",
      "Validation: Epoch average loss 0.182210\tAccuracy: 94.940000%\n",
      "Epoch 2\n",
      "[0/60000 (0%)] Training\tBatch loss: 0.008377\tAccuracy: 100.000000%\n",
      "[1600/60000 (3%)] Training\tBatch loss: 0.014886\tAccuracy: 84.715347%\n",
      "[3200/60000 (5%)] Training\tBatch loss: 0.039943\tAccuracy: 84.017413%\n",
      "[4800/60000 (8%)] Training\tBatch loss: 0.011182\tAccuracy: 84.053156%\n",
      "[6400/60000 (11%)] Training\tBatch loss: 0.032030\tAccuracy: 84.289277%\n",
      "[8000/60000 (13%)] Training\tBatch loss: 0.014941\tAccuracy: 84.281437%\n",
      "[9600/60000 (16%)] Training\tBatch loss: 0.019783\tAccuracy: 84.078619%\n",
      "[11200/60000 (19%)] Training\tBatch loss: 0.033985\tAccuracy: 83.978245%\n",
      "[12800/60000 (21%)] Training\tBatch loss: 0.024003\tAccuracy: 83.926342%\n",
      "[14400/60000 (24%)] Training\tBatch loss: 0.029833\tAccuracy: 83.788846%\n",
      "[16000/60000 (27%)] Training\tBatch loss: 0.033909\tAccuracy: 83.759990%\n",
      "[17600/60000 (29%)] Training\tBatch loss: 0.026874\tAccuracy: 83.969119%\n",
      "[19200/60000 (32%)] Training\tBatch loss: 0.031283\tAccuracy: 84.034138%\n",
      "[20800/60000 (35%)] Training\tBatch loss: 0.021028\tAccuracy: 84.214066%\n",
      "[22400/60000 (37%)] Training\tBatch loss: 0.021021\tAccuracy: 84.154176%\n",
      "[24000/60000 (40%)] Training\tBatch loss: 0.024109\tAccuracy: 84.239674%\n",
      "[25600/60000 (43%)] Training\tBatch loss: 0.035352\tAccuracy: 84.271549%\n",
      "[27200/60000 (45%)] Training\tBatch loss: 0.060547\tAccuracy: 84.296002%\n",
      "[28800/60000 (48%)] Training\tBatch loss: 0.014834\tAccuracy: 84.407968%\n",
      "[30400/60000 (51%)] Training\tBatch loss: 0.033230\tAccuracy: 84.504866%\n",
      "[32000/60000 (53%)] Training\tBatch loss: 0.047259\tAccuracy: 84.576462%\n",
      "[33600/60000 (56%)] Training\tBatch loss: 0.063707\tAccuracy: 84.682889%\n",
      "[35200/60000 (59%)] Training\tBatch loss: 0.026179\tAccuracy: 84.728532%\n",
      "[36800/60000 (61%)] Training\tBatch loss: 0.014225\tAccuracy: 84.753911%\n",
      "[38400/60000 (64%)] Training\tBatch loss: 0.020367\tAccuracy: 84.829238%\n",
      "[40000/60000 (67%)] Training\tBatch loss: 0.042780\tAccuracy: 84.866054%\n",
      "[41600/60000 (69%)] Training\tBatch loss: 0.018458\tAccuracy: 84.933679%\n",
      "[43200/60000 (72%)] Training\tBatch loss: 0.020683\tAccuracy: 84.998612%\n",
      "[44800/60000 (75%)] Training\tBatch loss: 0.040777\tAccuracy: 84.980810%\n",
      "[46400/60000 (77%)] Training\tBatch loss: 0.017646\tAccuracy: 84.979317%\n",
      "[48000/60000 (80%)] Training\tBatch loss: 0.038428\tAccuracy: 84.984172%\n",
      "[49600/60000 (83%)] Training\tBatch loss: 0.045878\tAccuracy: 84.982667%\n",
      "[51200/60000 (85%)] Training\tBatch loss: 0.024400\tAccuracy: 84.991018%\n",
      "[52800/60000 (88%)] Training\tBatch loss: 0.020862\tAccuracy: 84.981824%\n",
      "[54400/60000 (91%)] Training\tBatch loss: 0.016750\tAccuracy: 85.013599%\n",
      "[56000/60000 (93%)] Training\tBatch loss: 0.010915\tAccuracy: 85.018566%\n",
      "[57600/60000 (96%)] Training\tBatch loss: 0.028961\tAccuracy: 85.018051%\n",
      "[59200/60000 (99%)] Training\tBatch loss: 0.037420\tAccuracy: 85.037828%\n",
      "Training: Epoch average loss 0.031092 Epoch accuracy 85.051667%\n",
      "[0/10000 (0%)] Validation\tBatch loss: 0.000356\tAccuracy: 94.531250%\n",
      "[512/10000 (5%)] Validation\tBatch loss: 0.000427\tAccuracy: 94.531250%\n",
      "[1024/10000 (10%)] Validation\tBatch loss: 0.000487\tAccuracy: 94.205729%\n",
      "[1536/10000 (15%)] Validation\tBatch loss: 0.000451\tAccuracy: 94.091797%\n",
      "[2048/10000 (20%)] Validation\tBatch loss: 0.000480\tAccuracy: 93.945312%\n",
      "[2560/10000 (25%)] Validation\tBatch loss: 0.000452\tAccuracy: 93.847656%\n",
      "[3072/10000 (30%)] Validation\tBatch loss: 0.000436\tAccuracy: 93.833705%\n",
      "[3584/10000 (35%)] Validation\tBatch loss: 0.000510\tAccuracy: 93.750000%\n",
      "[4096/10000 (40%)] Validation\tBatch loss: 0.000406\tAccuracy: 93.771701%\n",
      "[4608/10000 (45%)] Validation\tBatch loss: 0.000349\tAccuracy: 93.906250%\n",
      "[5120/10000 (50%)] Validation\tBatch loss: 0.000175\tAccuracy: 94.247159%\n",
      "[5632/10000 (55%)] Validation\tBatch loss: 0.000347\tAccuracy: 94.303385%\n",
      "[6144/10000 (60%)] Validation\tBatch loss: 0.000305\tAccuracy: 94.456130%\n",
      "[6656/10000 (65%)] Validation\tBatch loss: 0.000167\tAccuracy: 94.740513%\n",
      "[7168/10000 (70%)] Validation\tBatch loss: 0.000195\tAccuracy: 94.973958%\n",
      "[7680/10000 (75%)] Validation\tBatch loss: 0.000297\tAccuracy: 95.019531%\n",
      "[8192/10000 (80%)] Validation\tBatch loss: 0.000195\tAccuracy: 95.151654%\n",
      "[8704/10000 (85%)] Validation\tBatch loss: 0.000258\tAccuracy: 95.258247%\n",
      "[9216/10000 (90%)] Validation\tBatch loss: 0.000213\tAccuracy: 95.415296%\n",
      "[5168/10000 (95%)] Validation\tBatch loss: 0.001031\tAccuracy: 95.310000%\n",
      "Validation: Epoch average loss 0.178146\tAccuracy: 95.310000%\n",
      "Epoch 3\n",
      "[0/60000 (0%)] Training\tBatch loss: 0.024334\tAccuracy: 87.500000%\n",
      "[1600/60000 (3%)] Training\tBatch loss: 0.013429\tAccuracy: 85.457921%\n",
      "[3200/60000 (5%)] Training\tBatch loss: 0.022511\tAccuracy: 86.287313%\n",
      "[4800/60000 (8%)] Training\tBatch loss: 0.013563\tAccuracy: 86.586379%\n",
      "[6400/60000 (11%)] Training\tBatch loss: 0.035040\tAccuracy: 86.502494%\n",
      "[8000/60000 (13%)] Training\tBatch loss: 0.017631\tAccuracy: 86.427146%\n",
      "[9600/60000 (16%)] Training\tBatch loss: 0.012752\tAccuracy: 86.335275%\n",
      "[11200/60000 (19%)] Training\tBatch loss: 0.040070\tAccuracy: 86.180456%\n",
      "[12800/60000 (21%)] Training\tBatch loss: 0.018937\tAccuracy: 86.056492%\n",
      "[14400/60000 (24%)] Training\tBatch loss: 0.037388\tAccuracy: 85.876804%\n",
      "[16000/60000 (27%)] Training\tBatch loss: 0.021877\tAccuracy: 85.739261%\n",
      "[17600/60000 (29%)] Training\tBatch loss: 0.011884\tAccuracy: 85.848093%\n",
      "[19200/60000 (32%)] Training\tBatch loss: 0.044457\tAccuracy: 85.735845%\n",
      "[20800/60000 (35%)] Training\tBatch loss: 0.033867\tAccuracy: 85.799385%\n",
      "[22400/60000 (37%)] Training\tBatch loss: 0.017250\tAccuracy: 85.849393%\n",
      "[24000/60000 (40%)] Training\tBatch loss: 0.015941\tAccuracy: 85.796969%\n",
      "[25600/60000 (43%)] Training\tBatch loss: 0.037423\tAccuracy: 85.864304%\n",
      "[27200/60000 (45%)] Training\tBatch loss: 0.028836\tAccuracy: 85.894327%\n",
      "[28800/60000 (48%)] Training\tBatch loss: 0.020220\tAccuracy: 85.993892%\n",
      "[30400/60000 (51%)] Training\tBatch loss: 0.025743\tAccuracy: 86.066544%\n",
      "[32000/60000 (53%)] Training\tBatch loss: 0.014717\tAccuracy: 86.172539%\n",
      "[33600/60000 (56%)] Training\tBatch loss: 0.020857\tAccuracy: 86.200024%\n",
      "[35200/60000 (59%)] Training\tBatch loss: 0.017787\tAccuracy: 86.227851%\n",
      "[36800/60000 (61%)] Training\tBatch loss: 0.031417\tAccuracy: 86.215233%\n",
      "[38400/60000 (64%)] Training\tBatch loss: 0.056150\tAccuracy: 86.211474%\n",
      "[40000/60000 (67%)] Training\tBatch loss: 0.014058\tAccuracy: 86.213015%\n",
      "[41600/60000 (69%)] Training\tBatch loss: 0.028760\tAccuracy: 86.226451%\n",
      "[43200/60000 (72%)] Training\tBatch loss: 0.023074\tAccuracy: 86.252777%\n",
      "[44800/60000 (75%)] Training\tBatch loss: 0.040369\tAccuracy: 86.281685%\n",
      "[46400/60000 (77%)] Training\tBatch loss: 0.006459\tAccuracy: 86.274130%\n",
      "[48000/60000 (80%)] Training\tBatch loss: 0.016165\tAccuracy: 86.302483%\n",
      "[49600/60000 (83%)] Training\tBatch loss: 0.017158\tAccuracy: 86.274589%\n",
      "[51200/60000 (85%)] Training\tBatch loss: 0.024942\tAccuracy: 86.285536%\n",
      "[52800/60000 (88%)] Training\tBatch loss: 0.048848\tAccuracy: 86.318540%\n",
      "[54400/60000 (91%)] Training\tBatch loss: 0.004192\tAccuracy: 86.323875%\n",
      "[56000/60000 (93%)] Training\tBatch loss: 0.030512\tAccuracy: 86.344973%\n",
      "[57600/60000 (96%)] Training\tBatch loss: 0.033950\tAccuracy: 86.351014%\n",
      "[59200/60000 (99%)] Training\tBatch loss: 0.027794\tAccuracy: 86.344907%\n",
      "Training: Epoch average loss 0.028732 Epoch accuracy 86.346667%\n",
      "[0/10000 (0%)] Validation\tBatch loss: 0.000343\tAccuracy: 95.703125%\n",
      "[512/10000 (5%)] Validation\tBatch loss: 0.000384\tAccuracy: 95.410156%\n",
      "[1024/10000 (10%)] Validation\tBatch loss: 0.000469\tAccuracy: 94.726562%\n",
      "[1536/10000 (15%)] Validation\tBatch loss: 0.000414\tAccuracy: 94.775391%\n",
      "[2048/10000 (20%)] Validation\tBatch loss: 0.000405\tAccuracy: 94.687500%\n",
      "[2560/10000 (25%)] Validation\tBatch loss: 0.000343\tAccuracy: 94.921875%\n",
      "[3072/10000 (30%)] Validation\tBatch loss: 0.000400\tAccuracy: 94.866071%\n",
      "[3584/10000 (35%)] Validation\tBatch loss: 0.000474\tAccuracy: 94.775391%\n",
      "[4096/10000 (40%)] Validation\tBatch loss: 0.000357\tAccuracy: 94.726562%\n",
      "[4608/10000 (45%)] Validation\tBatch loss: 0.000310\tAccuracy: 94.726562%\n",
      "[5120/10000 (50%)] Validation\tBatch loss: 0.000154\tAccuracy: 94.975142%\n",
      "[5632/10000 (55%)] Validation\tBatch loss: 0.000340\tAccuracy: 94.970703%\n",
      "[6144/10000 (60%)] Validation\tBatch loss: 0.000294\tAccuracy: 95.072115%\n",
      "[6656/10000 (65%)] Validation\tBatch loss: 0.000143\tAccuracy: 95.326451%\n",
      "[7168/10000 (70%)] Validation\tBatch loss: 0.000187\tAccuracy: 95.546875%\n",
      "[7680/10000 (75%)] Validation\tBatch loss: 0.000267\tAccuracy: 95.605469%\n",
      "[8192/10000 (80%)] Validation\tBatch loss: 0.000187\tAccuracy: 95.726103%\n",
      "[8704/10000 (85%)] Validation\tBatch loss: 0.000191\tAccuracy: 95.865885%\n",
      "[9216/10000 (90%)] Validation\tBatch loss: 0.000207\tAccuracy: 95.990954%\n",
      "[5168/10000 (95%)] Validation\tBatch loss: 0.000934\tAccuracy: 95.900000%\n",
      "Validation: Epoch average loss 0.160843\tAccuracy: 95.900000%\n",
      "Epoch 4\n",
      "[0/60000 (0%)] Training\tBatch loss: 0.019108\tAccuracy: 87.500000%\n",
      "[1600/60000 (3%)] Training\tBatch loss: 0.031340\tAccuracy: 85.643564%\n",
      "[3200/60000 (5%)] Training\tBatch loss: 0.026739\tAccuracy: 86.194030%\n",
      "[4800/60000 (8%)] Training\tBatch loss: 0.008162\tAccuracy: 86.337209%\n",
      "[6400/60000 (11%)] Training\tBatch loss: 0.056022\tAccuracy: 86.549252%\n",
      "[8000/60000 (13%)] Training\tBatch loss: 0.012366\tAccuracy: 86.452096%\n",
      "[9600/60000 (16%)] Training\tBatch loss: 0.026612\tAccuracy: 86.283278%\n",
      "[11200/60000 (19%)] Training\tBatch loss: 0.025456\tAccuracy: 86.688659%\n",
      "[12800/60000 (21%)] Training\tBatch loss: 0.027623\tAccuracy: 86.727528%\n",
      "[14400/60000 (24%)] Training\tBatch loss: 0.024144\tAccuracy: 86.570477%\n",
      "[16000/60000 (27%)] Training\tBatch loss: 0.007641\tAccuracy: 86.600899%\n",
      "[17600/60000 (29%)] Training\tBatch loss: 0.042612\tAccuracy: 86.637148%\n",
      "[19200/60000 (32%)] Training\tBatch loss: 0.053932\tAccuracy: 86.708993%\n",
      "[20800/60000 (35%)] Training\tBatch loss: 0.039327\tAccuracy: 86.654497%\n",
      "[22400/60000 (37%)] Training\tBatch loss: 0.026382\tAccuracy: 86.652391%\n",
      "[24000/60000 (40%)] Training\tBatch loss: 0.021538\tAccuracy: 86.679714%\n",
      "[25600/60000 (43%)] Training\tBatch loss: 0.015685\tAccuracy: 86.695815%\n",
      "[27200/60000 (45%)] Training\tBatch loss: 0.039753\tAccuracy: 86.669606%\n",
      "[28800/60000 (48%)] Training\tBatch loss: 0.055358\tAccuracy: 86.677540%\n",
      "[30400/60000 (51%)] Training\tBatch loss: 0.030107\tAccuracy: 86.701078%\n",
      "[32000/60000 (53%)] Training\tBatch loss: 0.029220\tAccuracy: 86.694153%\n",
      "[33600/60000 (56%)] Training\tBatch loss: 0.041136\tAccuracy: 86.673013%\n",
      "[35200/60000 (59%)] Training\tBatch loss: 0.043181\tAccuracy: 86.707746%\n",
      "[36800/60000 (61%)] Training\tBatch loss: 0.037683\tAccuracy: 86.625380%\n",
      "[38400/60000 (64%)] Training\tBatch loss: 0.066811\tAccuracy: 86.661808%\n",
      "[40000/60000 (67%)] Training\tBatch loss: 0.009488\tAccuracy: 86.725310%\n",
      "[41600/60000 (69%)] Training\tBatch loss: 0.007497\tAccuracy: 86.709439%\n",
      "[43200/60000 (72%)] Training\tBatch loss: 0.037351\tAccuracy: 86.720196%\n",
      "[44800/60000 (75%)] Training\tBatch loss: 0.006476\tAccuracy: 86.748036%\n",
      "[46400/60000 (77%)] Training\tBatch loss: 0.035061\tAccuracy: 86.733023%\n",
      "[48000/60000 (80%)] Training\tBatch loss: 0.052908\tAccuracy: 86.771076%\n",
      "[49600/60000 (83%)] Training\tBatch loss: 0.023566\tAccuracy: 86.730087%\n",
      "[51200/60000 (85%)] Training\tBatch loss: 0.010883\tAccuracy: 86.785380%\n",
      "[52800/60000 (88%)] Training\tBatch loss: 0.014067\tAccuracy: 86.807028%\n",
      "[54400/60000 (91%)] Training\tBatch loss: 0.029114\tAccuracy: 86.814540%\n",
      "[56000/60000 (93%)] Training\tBatch loss: 0.042443\tAccuracy: 86.805556%\n",
      "[57600/60000 (96%)] Training\tBatch loss: 0.013682\tAccuracy: 86.804013%\n",
      "[59200/60000 (99%)] Training\tBatch loss: 0.029991\tAccuracy: 86.827884%\n",
      "Training: Epoch average loss 0.027624 Epoch accuracy 86.861667%\n",
      "[0/10000 (0%)] Validation\tBatch loss: 0.000279\tAccuracy: 95.703125%\n",
      "[512/10000 (5%)] Validation\tBatch loss: 0.000370\tAccuracy: 94.726562%\n",
      "[1024/10000 (10%)] Validation\tBatch loss: 0.000414\tAccuracy: 94.466146%\n",
      "[1536/10000 (15%)] Validation\tBatch loss: 0.000348\tAccuracy: 94.531250%\n",
      "[2048/10000 (20%)] Validation\tBatch loss: 0.000407\tAccuracy: 94.492188%\n",
      "[2560/10000 (25%)] Validation\tBatch loss: 0.000312\tAccuracy: 94.628906%\n",
      "[3072/10000 (30%)] Validation\tBatch loss: 0.000349\tAccuracy: 94.810268%\n",
      "[3584/10000 (35%)] Validation\tBatch loss: 0.000397\tAccuracy: 94.848633%\n",
      "[4096/10000 (40%)] Validation\tBatch loss: 0.000335\tAccuracy: 94.791667%\n",
      "[4608/10000 (45%)] Validation\tBatch loss: 0.000277\tAccuracy: 94.882812%\n",
      "[5120/10000 (50%)] Validation\tBatch loss: 0.000150\tAccuracy: 95.134943%\n",
      "[5632/10000 (55%)] Validation\tBatch loss: 0.000295\tAccuracy: 95.166016%\n",
      "[6144/10000 (60%)] Validation\tBatch loss: 0.000280\tAccuracy: 95.252404%\n",
      "[6656/10000 (65%)] Validation\tBatch loss: 0.000121\tAccuracy: 95.507812%\n",
      "[7168/10000 (70%)] Validation\tBatch loss: 0.000139\tAccuracy: 95.729167%\n",
      "[7680/10000 (75%)] Validation\tBatch loss: 0.000186\tAccuracy: 95.812988%\n",
      "[8192/10000 (80%)] Validation\tBatch loss: 0.000198\tAccuracy: 95.818015%\n",
      "[8704/10000 (85%)] Validation\tBatch loss: 0.000173\tAccuracy: 95.941840%\n",
      "[9216/10000 (90%)] Validation\tBatch loss: 0.000157\tAccuracy: 96.083470%\n",
      "[5168/10000 (95%)] Validation\tBatch loss: 0.000759\tAccuracy: 96.020000%\n",
      "Validation: Epoch average loss 0.141627\tAccuracy: 96.020000%\n",
      "Epoch 5\n",
      "[0/60000 (0%)] Training\tBatch loss: 0.037950\tAccuracy: 75.000000%\n",
      "[1600/60000 (3%)] Training\tBatch loss: 0.024747\tAccuracy: 88.675743%\n",
      "[3200/60000 (5%)] Training\tBatch loss: 0.034032\tAccuracy: 87.748756%\n",
      "[4800/60000 (8%)] Training\tBatch loss: 0.034760\tAccuracy: 87.624585%\n",
      "[6400/60000 (11%)] Training\tBatch loss: 0.020234\tAccuracy: 87.515586%\n",
      "[8000/60000 (13%)] Training\tBatch loss: 0.026217\tAccuracy: 87.549900%\n",
      "[9600/60000 (16%)] Training\tBatch loss: 0.014203\tAccuracy: 87.614393%\n",
      "[11200/60000 (19%)] Training\tBatch loss: 0.016817\tAccuracy: 87.624822%\n",
      "[12800/60000 (21%)] Training\tBatch loss: 0.015899\tAccuracy: 87.788702%\n",
      "[14400/60000 (24%)] Training\tBatch loss: 0.006577\tAccuracy: 87.728912%\n",
      "[16000/60000 (27%)] Training\tBatch loss: 0.040333\tAccuracy: 87.662338%\n",
      "[17600/60000 (29%)] Training\tBatch loss: 0.029349\tAccuracy: 87.573797%\n",
      "[19200/60000 (32%)] Training\tBatch loss: 0.041709\tAccuracy: 87.463572%\n",
      "[20800/60000 (35%)] Training\tBatch loss: 0.011550\tAccuracy: 87.514412%\n",
      "[22400/60000 (37%)] Training\tBatch loss: 0.017206\tAccuracy: 87.566916%\n",
      "[24000/60000 (40%)] Training\tBatch loss: 0.021924\tAccuracy: 87.508328%\n",
      "[25600/60000 (43%)] Training\tBatch loss: 0.023083\tAccuracy: 87.539038%\n",
      "[27200/60000 (45%)] Training\tBatch loss: 0.032548\tAccuracy: 87.566138%\n",
      "[28800/60000 (48%)] Training\tBatch loss: 0.039161\tAccuracy: 87.548584%\n",
      "[30400/60000 (51%)] Training\tBatch loss: 0.022600\tAccuracy: 87.598632%\n",
      "[32000/60000 (53%)] Training\tBatch loss: 0.016509\tAccuracy: 87.556222%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-ddc7f50da47b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Epoch {}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m         \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_loader\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m         \u001B[0mtest\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mval_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-6-c2b9d84c44f5>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, optimizer)\u001B[0m\n\u001B[1;32m     45\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# zero the accumulated gradients\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 47\u001B[0;31m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# computer network's output\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     48\u001B[0m         \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcross_entropy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# computer the loss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m         \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;31m# backward pass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/pr/ai_course/lab_1/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    720\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    721\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 722\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    723\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    724\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-6-c2b9d84c44f5>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mview\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/pr/ai_course/lab_1/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    720\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    721\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 722\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    723\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    724\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/pr/ai_course/lab_1/venv/lib/python3.8/site-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/pr/ai_course/lab_1/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    720\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    721\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 722\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    723\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    724\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/pr/ai_course/lab_1/venv/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    129\u001B[0m         \u001B[0mused\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mnormalization\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m \u001B[0;32min\u001B[0m \u001B[0meval\u001B[0m \u001B[0mmode\u001B[0m \u001B[0mwhen\u001B[0m \u001B[0mbuffers\u001B[0m \u001B[0mare\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m         \"\"\"\n\u001B[0;32m--> 131\u001B[0;31m         return F.batch_norm(\n\u001B[0m\u001B[1;32m    132\u001B[0m             \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    133\u001B[0m             \u001B[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/pr/ai_course/lab_1/venv/lib/python3.8/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mbatch_norm\u001B[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[1;32m   2012\u001B[0m         \u001B[0m_verify_batch_size\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2013\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2014\u001B[0;31m     return torch.batch_norm(\n\u001B[0m\u001B[1;32m   2015\u001B[0m         \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrunning_mean\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrunning_var\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2016\u001B[0m         \u001B[0mtraining\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmomentum\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meps\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackends\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcudnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menabled\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# loader of the training set\n",
    "train_loader = torch.utils.data.DataLoader(mnist_dataset_train,batch_size=16, shuffle=True)\n",
    "# loader of the validation set\n",
    "val_loader = torch.utils.data.DataLoader(mnist_dataset_val,batch_size=512, shuffle=False)\n",
    "\n",
    "model = MnistNet() # initialize network structure\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(1, 20 + 1):\n",
    "        print('Epoch {}'.format(epoch))\n",
    "        train(model, train_loader, optimizer)\n",
    "        test(model, val_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-dd0e9a37",
   "language": "python",
   "display_name": "PyCharm (lab_3_neural_networks)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}